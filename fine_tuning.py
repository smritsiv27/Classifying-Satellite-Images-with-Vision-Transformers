# -*- coding: utf-8 -*-
"""fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wy8e39IwtEsD5hqwLg4i7d_gM1MiNgLL

# xBD Working Notebook

Brief description of project (to add)
"""

!pip install -q datasets transformers accelerate evaluate rasterio
#!pip install wandb
!pip install ujson

"""# Configuration and Imports

Config and imports
"""

#token for huggingface: hf_XXXXXXXXXXXXXXXXXX
from huggingface_hub import notebook_login
notebook_login()

#Configuration
#%%capture
#!sudo apt -qq install git-lfs
#!git config --global credential.helper store

#from transformers.utils import send_example_telemetry
#send_example_telemetry("image_classification_notebook", framework="pytorch")

#Imports
import json
from shapely import wkt
from shapely.geometry import Polygon
from PIL import Image
import numpy as np
import torch
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader, IterableDataset, random_split
from transformers import AutoImageProcessor, ViTForImageClassification, AutoModelForImageClassification, TrainingArguments, Trainer, EarlyStoppingCallback
import os
from google.colab import drive
import random
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
#import json
from pathlib import Path
from shapely import wkt
import math
import time
from functools import lru_cache
import evaluate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import ujson
import csv
from transformers import TrainerCallback
###
from transformers import AutoFeatureExtractor, ViTForImageClassification
import requests

import warnings
from sklearn.exceptions import UndefinedMetricWarning

warnings.filterwarnings("ignore", category=UndefinedMetricWarning)

# Commented out IPython magic to ensure Python compatibility.
#Mount data directory
drive.mount('/content/drive')
# %cd <Local Drive with the data>
!pwd

#root_folder = set_root_folder_here

"""# Custom Dataloader Class

Explain what this is trying to do (preproccessing logic)
"""

# @title Default title text
class EfficientXBDDataset(Dataset):
    def __init__(self, images_root, labels_root, image_processor,
                 patch_size=224, shift_param=50, label_map=None):
        self.images_root = Path(images_root)
        self.labels_root = Path(labels_root)
        self.image_processor = image_processor
        self.patch_size = patch_size
        self.shift_param = shift_param
        self.label_map = label_map or {
            "no-damage": 0,
            "minor-damage": 1,
            "major-damage": 2,
            "destroyed": 3,
        }
        self.image_files = sorted(self.images_root.rglob("*.png"))
        self.label_files = [self.labels_root / img_path.relative_to(self.images_root).with_suffix('.json')
                            for img_path in self.image_files]

    def crop_patch(self, img, centroid):
        x, y = int(centroid[0]), int(centroid[1])
        h, w = img.shape[0], img.shape[1]
        half = self.patch_size // 2
        if self.shift_param > 0:
            x += random.randint(-self.shift_param, self.shift_param)
            y += random.randint(-self.shift_param, self.shift_param)
        x1 = max(0, min(x - half, w - self.patch_size))
        y1 = max(0, min(y - half, h - self.patch_size))
        x2 = x1 + self.patch_size
        y2 = y1 + self.patch_size
        patch = img[y1:y2, x1:x2, :]
        return patch

    def crop_and_resize_patch(self, img, centroid, search_radius=100, stride=10):
      h, w = img.shape[:2]
      half = self.patch_size // 2
      x_center, y_center = int(centroid[0]), int(centroid[1])
      min_empty_ratio = float('inf')
      best_coords = (0, 0)
      x_start = max(0, x_center - half - search_radius)
      x_end = min(w - self.patch_size, x_center - half + search_radius)
      y_start = max(0, y_center - half - search_radius)
      y_end = min(h - self.patch_size, y_center - half + search_radius)
      for x1 in range(x_start, x_end + 1, stride):
          for y1 in range(y_start, y_end + 1, stride):
              x2 = x1 + self.patch_size
              y2 = y1 + self.patch_size
              patch = img[y1:y2, x1:x2, :]
              rgb_patch = patch[:, :, :3]
              alpha_patch = patch[:, :, 3]
              black_pixels = np.all(rgb_patch <= 10, axis=2)
              transparent_pixels = (alpha_patch == 0)
              empty_pixels = np.logical_or(black_pixels, transparent_pixels)
              empty_ratio = np.sum(empty_pixels) / (self.patch_size * self.patch_size)
              if empty_ratio < min_empty_ratio:
                  min_empty_ratio = empty_ratio
                  best_coords = (x1, y1)
              if min_empty_ratio == 0:
                  break
          if min_empty_ratio == 0:
              break
      x1, y1 = best_coords
      x2 = x1 + self.patch_size
      y2 = y1 + self.patch_size
      best_patch = img[y1:y2, x1:x2, :]
      patch_img = Image.fromarray(best_patch)
      if patch_img.size != (self.patch_size, self.patch_size):
          patch_img = patch_img.resize((self.patch_size, self.patch_size), Image.BILINEAR)
      return np.array(patch_img)

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        #Start the getitem stopwatch
        #start_time = time.time()
        #last_time = start_time
        img_path = self.image_files[idx]
        label_path = self.label_files[idx]
        if not label_path.exists():
            return None
        # try:
        pil_img = Image.open(img_path)
        img = np.array(pil_img)

        ###### SLOW BIT HERE ######
        # except Exception:
        #     return None
        if img.shape[2] == 3:
            alpha_channel = 255 * np.ones((img.shape[0], img.shape[1], 1), dtype=np.uint8)
            #img = np.concatenate([img, alpha_channel], axis=2)
            img = np.dstack((img, alpha_channel)) #To make it faster
        # try:
        ###### SLOW BIT HERE ######
        #Faster with ujson
        with open(label_path, "r") as f:
            label_data = ujson.load(f)
        buildings = label_data.get("features", {}).get("xy", [])
        #with open(label_path, 'r') as f:
        #    label_data = json.load(f)
        #buildings = label_data.get("features", {}).get("xy", [])
        # except Exception:
        #     return None
        valid_buildings = [b for b in buildings if b.get("properties", {}).get("subtype") in self.label_map]
        if len(valid_buildings) == 0:
            return None
        ###### up to here ######

        building = random.choice(valid_buildings)
        label = self.label_map[building["properties"]["subtype"]]
        polygon = wkt.loads(building["wkt"])
        centroid = polygon.centroid.coords[0]
        patch = self.crop_patch(img, centroid)
        alpha_channel = patch[:, :, 3]
        num_transparent = np.sum(alpha_channel == 0)
        total_pixels = alpha_channel.size
        transparent_ratio = num_transparent / total_pixels
        rgb_patch = patch[:, :, :3]
        black_threshold = 10
        black_pixels_mask = np.all(rgb_patch <= black_threshold, axis=2)
        black_ratio = np.sum(black_pixels_mask) / total_pixels
        threshold = 0.01
        if transparent_ratio > threshold or black_ratio > threshold:
            #print('cropping due to transparency or black pixels')
            patch = self.crop_and_resize_patch(img, centroid)
        patch_rgb = patch[:, :, :3]
        image = Image.fromarray(patch_rgb.astype(np.uint8))
        output_crop = self.image_processor(images=image, return_tensors="pt")['pixel_values'][0]
        return {"pixel_values": output_crop, "labels": label}


class BatchIterableDataset(torch.utils.data.IterableDataset):
    def __init__(self, base_dataset, batch_size, shuffle=False):
        self.base_dataset = base_dataset
        self.batch_size = batch_size
        self.shuffle = shuffle

    def __len__(self):
        return (len(self.base_dataset) + self.batch_size - 1) // self.batch_size

    def __iter__(self):
        indices = list(range(len(self.base_dataset)))
        if self.shuffle:
            random.shuffle(indices)

        batch = []
        for idx in indices:
            sample = self.base_dataset[idx]
            if sample is None:
                continue
            batch.append(sample)
            if len(batch) == self.batch_size:
                yield {
                    "pixel_values": torch.stack([x["pixel_values"] for x in batch]),
                    "labels": torch.tensor([x["labels"] for x in batch])
                }
                batch = []

        # yield last batch if not empty
        if batch:
            yield {
                "pixel_values": torch.stack([x["pixel_values"] for x in batch]),
                "labels": torch.tensor([x["labels"] for x in batch])
            }

"""# Training"""

class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
      if self.label_smoother is not None and "labels" in inputs:
          labels = inputs.pop("labels")
      else:
          labels = inputs.get("labels", None)
      outputs = model(**inputs)

      # Extract scalar loss safely
      if hasattr(outputs, "loss") and outputs.loss is not None:
          loss = outputs.loss
      elif isinstance(outputs, dict) and "loss" in outputs:
          loss = outputs["loss"]
      else:
          loss = outputs[0]  # fallback

      # If label smoother is applied, compute the smoothed loss scalar
      if labels is not None and self.label_smoother is not None:
          loss = self.label_smoother(outputs, labels)

      # Ensure loss is a scalar tensor
      if loss.dim() != 0:
          loss = loss.mean()

      if labels is not None:
          preds = outputs.logits.detach()
          preds_np = preds.argmax(dim=1).cpu().numpy()
          labels_np = labels.cpu().numpy()
          acc1 = accuracy_score(labels_np, preds_np)
          self.log({'accuracy_score': acc1})
          #ADD OTHER METRICS NEXT
          prec = precision_score(labels_np, preds_np, average="weighted", zero_division=0)
          self.log({'precision_score': prec})
          rec = recall_score(labels_np, preds_np, average="weighted", zero_division=0)
          self.log({'recall_score': rec})
          f1 = f1_score(labels_np, preds_np, average="weighted", zero_division=0)
          self.log({'f1_score': f1})
          acc = (preds.argmax(dim=-1) == labels).float().mean().item()
          self.log({"train_accuracy": acc})

      if self.args.past_index >= 0:
          self._past = outputs[self.args.past_index]

      return (loss, outputs) if return_outputs else loss

#model_checkpoint = "facebook/dinov2-small"
model_checkpoint = "facebook/deit-tiny-patch16-224"
train_batch_size = 8
val_batch_size = 64

image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)

# Dynamically determine the target image size from the image processor
# This ensures the dataset's patch size matches what the model expects.
# DINOv2 image processor has a 'size' dictionary with 'shortest_edge' or 'height'
target_image_size = image_processor.size.get("shortest_edge", image_processor.size.get("height"))

images_root = root_folder + "/train/images/"
labels_root = root_folder + "/train/labels/"
train_dataset_unbatched = EfficientXBDDataset(images_root, labels_root, image_processor, patch_size=target_image_size)
dataset_size = len(train_dataset_unbatched)
train_size = int(0.8 * dataset_size)
val_size = dataset_size - train_size
train_subset, val_subset = random_split(train_dataset_unbatched, [train_size, val_size])
train_dataset = BatchIterableDataset(train_subset, train_batch_size, shuffle=True)
val_dataset = BatchIterableDataset(val_subset, val_batch_size, shuffle=False)
# split up training into training + validation

train_dataloader = DataLoader(
    train_dataset,
    batch_size=None,
    collate_fn=None)

val_dataloader = DataLoader(
    val_dataset,
    batch_size=None,
    collate_fn=None)

print(len(train_subset))
print(len(val_subset))

from transformers import Dinov2ForImageClassification, AutoImageProcessor

num_labels = 4  #for xBD: no-damage, minor-damage, major-damage, destroyed
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# Use Dinov2ForImageClassification, which is the specific classification head for DINOv2.
# This ensures the model is loaded with its correct configuration and expected input size.
model = Dinov2ForImageClassification.from_pretrained(
    model_checkpoint,
    num_labels=num_labels,
)
model = model.to(device)

"""**Freezing Pretrained Weights**"""

"""
UNCOMMENT THIS WHOLE BLOCK FOR END-TO-END TRAINING
"""


# Freeze everything
for param in model.parameters():
    param.requires_grad = False

# Unfreeze the classifier head
for param in model.classifier.parameters():
    param.requires_grad = True

print("Trainable params:")
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)

num_epochs = 4
train_batch_size = 24 #remember to change batch size in data loading too
eval_batch_size = 64 #remember to change batch size in data loading too
learning_rate = 5e-5
weight_decay = 0.05
num_devices = max(1, torch.cuda.device_count())
dataset_len = len(train_subset)
max_steps = math.ceil(dataset_len / (train_batch_size * num_devices) * num_epochs)
early_stop = EarlyStoppingCallback(early_stopping_patience=3)

print(f"Calculated max_steps = {max_steps}")

accuracy_metric = evaluate.load("accuracy")
#ADD OTHER METRICS NEXT
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):
        logits = logits[0]
    predictions = np.argmax(logits, axis=-1)

    acc = accuracy_metric.compute(predictions=predictions, references=labels)["accuracy"]
    precision = precision_metric.compute(predictions=predictions, references=labels, average="weighted")["precision"]
    recall = recall_metric.compute(predictions=predictions, references=labels, average="weighted")["recall"]
    f1 = f1_metric.compute(predictions=predictions, references=labels, average="weighted")["f1"]

    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

training_args = TrainingArguments(
    output_dir="root_folder + "/models/checkpoints2",
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=eval_batch_size,
    #gradient_accumulation_steps=5, #accumulate gradient over multiple batches
    gradient_checkpointing=False, #not storing all intermediate activations
    fp16=True,
    eval_strategy="steps",
    eval_steps=20,
    save_strategy="epoch",
    num_train_epochs=num_epochs,
    weight_decay=weight_decay,
    learning_rate=learning_rate,
    logging_dir="./logs",
    logging_strategy="steps",
    max_steps=max_steps
    #report_to="wandb",  # enable logging to W&B
    #run_name="DINO-V2-Tests",  # name of the W&B run (optional)
    #logging_steps=1,  # how often to log to W&B
)

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=lambda x: x[0],
    compute_metrics=compute_metrics,
    #callbacks=[early_stop] --- try this next
)

from google.colab import runtime
import pandas as pd
#API key: XXXXXXXXXXXXXXXXXXXXX

trainer.train()
print("training completed.")
trainer.save_model(root_folder + "/models/Dec4_24_4_5e5")
print("training model saved.")

# Read your logs
log_history = trainer.state.log_history
df_logs = pd.DataFrame(log_history)

# REMOVE NA CELLS AND CREATE ONE ROW
df_grouped = (
    df_logs.groupby('epoch')
      .agg(lambda x: next((i for i in x if pd.notnull(i)), None))
      .reset_index()
)

#print(df_grouped)
# Save to CSV
#df_logs.to_csv(root_folder + "/training_log_history_NovXX.csv", index=False) #not needed to check any more.
df_grouped.to_csv(root_folder + "/training_logs/training_log_history_Dec4_24_4_5e5_Frozen", index=False)
print("Training logs saved.")

runtime.unassign()



import matplotlib.pyplot as plt
from torchvision.transforms import ToPILImage
import torch # Import torch

def sanity_check_dataloader(dataloader, num_batches_to_check=2, show_samples_per_batch=2):
    to_pil = ToPILImage()
    print(f"Sanity checking dataloader...")

    for i, batch in enumerate(dataloader):
        if i >= num_batches_to_check:
            break

        print(f"\nChecking batch {i+1}:")
        print(f"Type of batch: {type(batch)}")

        if isinstance(batch, dict):
            print(f"Keys in batch: {batch.keys()}")
            if "pixel_values" in batch and "labels" in batch: # Expecting 'labels' based on BatchIterableDataset fix
                pixel_values = batch["pixel_values"]
                labels = batch["labels"]

                print("Batch 'pixel_values' shape:", pixel_values.shape)
                print("Batch 'labels' shape:", labels.shape)
                print(f"Actual batch size in this batch: {pixel_values.shape[0]}")

                # Check batch dimensions
                if len(pixel_values.shape) != 4:
                    print(f"Warning: Expected pixel_values to have 4 dimensions (B,C,H,W), got {pixel_values.shape}")
                if len(labels.shape) != 1 or labels.shape[0] != pixel_values.shape[0]:
                     print(f"Warning: Labels shape must be 1D and match batch size, got {labels.shape}")

                # Display sample images with labels
                for j in range(min(show_samples_per_batch, pixel_values.shape[0])):
                    img_tensor = pixel_values[j]
                    label = labels[j].item()
                    # Assuming ImageNet normalization, unnormalize for display
                    imagenet_mean = [0.485, 0.456, 0.406]
                    imagenet_std = [0.229, 0.224, 0.225]
                    # Ensure img_tensor is on CPU before unnormalizing and converting to PIL
                    img_tensor = img_tensor.cpu().detach().clone()
                    mean = torch.tensor(imagenet_mean).view(-1, 1, 1)
                    std = torch.tensor(imagenet_std).view(-1, 1, 1)
                    img_tensor = img_tensor * std + mean
                    img_tensor = img_tensor.clamp(0, 1) # Clamp values to [0, 1]

                    pil_img = to_pil(img_tensor)
                    plt.imshow(pil_img)
                    plt.title(f"Batch {i+1} Sample {j+1} - Label: {label}")
                    plt.axis('off')
                    plt.show()
            else:
                print("Batch is a dictionary but missing 'pixel_values' or 'labels' keys.")
        else:
            print("Batch is not a dictionary.")

    print("\nSanity check finished.")

# Usage for your dataloaders
sanity_check_dataloader(train_dataloader, num_batches_to_check=1) # Check 1 batch of training data
sanity_check_dataloader(val_dataloader, num_batches_to_check=1)   # Check 1 batch of validation data

# 1) Load the saved model (huggingface with link to file name in folder) <tensor weights (model.safetensors) # https://huggingface.co/docs/transformers/en/models
# e.g. load from (root_folder + "/models/Nov24_24_5_1e5")

from transformers import AutoConfig, AutoModelForImageClassification
import torch

model_dir = root_folder + "/models/Dec2_24_10_1e3_Frozen"

# Load config, then model with weights
config = AutoConfig.from_pretrained(model_dir)
model = AutoModelForImageClassification.from_pretrained(
    model_dir,
    config=config,
    trust_remote_code=True
)

model.eval().cuda()

model_checkpoint_for_processing = "facebook/deit-tiny-patch16-224"
image_processor = AutoImageProcessor.from_pretrained(model_checkpoint_for_processing)
images_root = root_folder + "/test/images/"
labels_root = root_folder + "/test/labels/"
test_subset = EfficientXBDDataset(images_root, labels_root, image_processor)
test_dataset_size = len(test_subset)
test_dataset = BatchIterableDataset(test_subset, test_dataset_size, shuffle=True)

from google.colab import runtime
eval_dataloader = DataLoader(
    test_dataset,
    batch_size=None,
    collate_fn=None)

num_labels = 4  #for xBD: no-damage, minor-damage, major-damage, destroyed
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

'''
all_preds = []
all_labels = []

import torch
from tqdm import tqdm

with torch.no_grad():
    for batch in tqdm(eval_dataloader):
        pixel_values = batch["pixel_values"].cuda()
        labels = batch["labels"].cuda()

        outputs = model(pixel_values)
        preds = outputs.logits.argmax(dim=-1)
        all_preds.append(preds.cpu().item())
        all_labels.append(labels.cpu().item())

        all_preds.extend(preds.cpu().tolist())
        all_labels.extend(labels.cpu().tolist())
'''


all_preds = []
all_labels = []

import torch
from tqdm import tqdm

N_RUNS = 5

model.eval()

for run in range(N_RUNS):
    print(f"\n=== EVAL RUN {run+1}/{N_RUNS} ===")

    with torch.no_grad():
        for batch in tqdm(eval_dataloader):
            pixel_values = batch["pixel_values"].cuda()
            labels = batch["labels"].cuda()

            outputs = model(pixel_values)
            preds = outputs.logits.argmax(dim=-1)

            all_preds.extend(preds.cpu().tolist())
            all_labels.extend(labels.cpu().tolist())

# 3) Accuracy, f1, precision



from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(all_labels, all_preds)
precision = precision_score(all_labels, all_preds, average="macro")
recall = recall_score(all_labels, all_preds, average="macro")
f1 = f1_score(all_labels, all_preds, average="macro")

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

print(all_labels)

print(all_preds)

from collections import Counter
freq = Counter(all_labels)
print(freq)

freq = Counter(all_preds)
print(freq)

import pandas as pd
df = pd.crosstab(pd.Series(all_labels), pd.Series(all_preds), rownames=['Actual'], colnames=['Predicted'])
print(df)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# cm: 2D numpy array with your confusion matrix values
# class_names: list of class labels, length = cm.shape[0]
def plot_confusion_matrix(cm, class_names, title="Confusion Matrix"):
    plt.figure(figsize=(6, 5))
    ax = sns.heatmap(
        cm,
        annot=True,          # show numbers
        fmt="g",             # integer format or ".2f" for floats
        cmap="Blues",        # color map
        cbar=False,          # hide color bar if you want
        square=True,
        linewidths=0.5,
        linecolor="gray",
        annot_kws={"fontsize": 10}
    )
    ax.set_xlabel("Predicted label", fontsize=12)
    ax.set_ylabel("True label", fontsize=12)
    ax.set_xticklabels(class_names, rotation=45, ha="right", fontsize=10)
    ax.set_yticklabels(class_names, rotation=0, fontsize=10)
    ax.set_title(title, fontsize=14, pad=12)
    plt.tight_layout()
    plt.show()

# example call:

cm = np.array([[2050, 32, 122, 54], [208, 12, 82, 18], [106, 17, 370, 34], [141, 23, 137, 289]])
plot_confusion_matrix(cm, ["class 0", "class 1", "class 2", "class 3"])